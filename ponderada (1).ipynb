{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bernardo Meirelles - BoW usando biblioteca\n",
        "## Está primeira parte é um BoW feito de maneira automatica com BoW, fiz apenas para aprender e ter noção de como usar no projeto com a biblioteca.\n",
        "## Abaixo teremos uma documentação completa de um BoW feito sme uso de NLtk, seguindo o padrão do kaggle de sklearn.\n",
        "#Modelo com biblioteca NLTK:"
      ],
      "metadata": {
        "id": "aXVaek2NYInQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Importação das bibliotecas\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, RSLPStemmer\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "cCBry28jJaQ9"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desenvolvimento de um Bag Of Words usando a biblioteca NLTK."
      ],
      "metadata": {
        "id": "jcTy2GW7YNKq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXiJ3ce4JBIn",
        "outputId": "02e8a493-01b1-4005-814a-e0805134b8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ],
      "source": [
        "# 2. Baixar recursos necessários do NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessar_ingles(texto):\n",
        "    # Converter para minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Remover pontuação e caracteres especiais\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    # Tokenização (usando o método correto para evitar o erro punkt_tab)\n",
        "    tokens = texto.split()\n",
        "\n",
        "    # Remover stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # Lematização\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "pi2RBXe7LLHD"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Função para pré-processar textos em português\n",
        "def preprocessar_portugues(texto):\n",
        "    # Converter para minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Remover pontuação e caracteres especiais\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    # Tokenização (usando o método correto para evitar o erro punkt_tab)\n",
        "    tokens = texto.split()\n",
        "\n",
        "    # Remover stopwords\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "    # Stemming para português\n",
        "    stemmer = RSLPStemmer()\n",
        "    tokens = [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "rbipSnSlLT_i"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 frases em inglês\n",
        "frases_ingles = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I love machine learning and natural language processing.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"Data science is an interdisciplinary field.\",\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"She enjoys reading books on history.\",\n",
        "    \"Open source software fosters collaboration.\",\n",
        "    \"Cloud computing provides scalable resources.\",\n",
        "    \"Big data analytics helps in decision making.\",\n",
        "    \"Neural networks are inspired by the human brain.\",\n",
        "    \"The movie was fantastic and thrilling.\",\n",
        "    \"He plays football every weekend.\",\n",
        "    \"Traveling broadens your horizons.\",\n",
        "    \"Healthy eating leads to a better life.\",\n",
        "    \"Education is the key to success.\",\n",
        "    \"Music can influence our emotions.\",\n",
        "    \"The company launched a new product.\",\n",
        "    \"I am learning to cook Italian cuisine.\",\n",
        "    \"Technology advances rapidly these days.\",\n",
        "    \"Space exploration inspires innovation.\",\n",
        "    \"Reading improves vocabulary and comprehension.\",\n",
        "    \"She is an expert in data visualization.\",\n",
        "    \"The garden is full of blooming flowers.\",\n",
        "    \"Meditation helps reduce stress.\"\n",
        "]\n",
        "\n",
        "# 25 frases em português\n",
        "frases_portugues = [\n",
        "    \"O rápido raposo marrom pula sobre o cão preguiçoso.\",\n",
        "    \"Eu amo aprendizado de máquina e processamento de linguagem natural.\",\n",
        "    \"Python é uma ótima linguagem de programação.\",\n",
        "    \"Ciência de dados é um campo interdisciplinar.\",\n",
        "    \"Inteligência artificial está transformando o mundo.\",\n",
        "    \"O tempo hoje está ensolarado e quente.\",\n",
        "    \"Ela gosta de ler livros sobre história.\",\n",
        "    \"Software de código aberto promove colaboração.\",\n",
        "    \"Computação em nuvem oferece recursos escaláveis.\",\n",
        "    \"Análise de big data ajuda na tomada de decisões.\",\n",
        "    \"Redes neurais são inspiradas no cérebro humano.\",\n",
        "    \"O filme foi fantástico e emocionante.\",\n",
        "    \"Ele joga futebol todo fim de semana.\",\n",
        "    \"Viajar amplia seus horizontes.\",\n",
        "    \"Alimentação saudável leva a uma vida melhor.\",\n",
        "    \"Educação é a chave para o sucesso.\",\n",
        "    \"A música pode influenciar nossas emoções.\",\n",
        "    \"A empresa lançou um novo produto.\",\n",
        "    \"Estou aprendendo a cozinhar culinária italiana.\",\n",
        "    \"A tecnologia avança rapidamente hoje em dia.\",\n",
        "    \"Exploração espacial inspira inovação.\",\n",
        "    \"Ler melhora vocabulário e compreensão.\",\n",
        "    \"Ela é especialista em visualização de dados.\",\n",
        "    \"O jardim está cheio de flores desabrochando.\",\n",
        "    \"Meditação ajuda a reduzir o estresse.\"\n",
        "]"
      ],
      "metadata": {
        "id": "xGcpgOovLOat"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Pré-processamento\n",
        "ingles_processado = [preprocessar_ingles(frase) for frase in frases_ingles]\n",
        "portugues_processado = [preprocessar_portugues(frase) for frase in frases_portugues]\n"
      ],
      "metadata": {
        "id": "_XGcQOMBWgOU"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7. Bag-of-Words para inglês\n",
        "print(\"\\n--- Bag of Words - Inglês ---\")\n",
        "vetorizador_en = CountVectorizer()\n",
        "X_en = vetorizador_en.fit_transform(ingles_processado)\n",
        "print(\"Vocabulário inglês:\", vetorizador_en.get_feature_names_out())\n",
        "print(\"Matriz BoW (dimensão):\", X_en.shape)\n",
        "print(pd.DataFrame(X_en.toarray(), columns=vetorizador_en.get_feature_names_out()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTYaEcQNMVne",
        "outputId": "b5180e0c-18b9-4ad8-fb82-fe1f0514c8cd"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bag of Words - Inglês ---\n",
            "Vocabulário inglês: ['advance' 'analytics' 'artificial' 'better' 'big' 'blooming' 'book'\n",
            " 'brain' 'broadens' 'brown' 'cloud' 'collaboration' 'company'\n",
            " 'comprehension' 'computing' 'cook' 'cuisine' 'data' 'day' 'decision'\n",
            " 'dog' 'eating' 'education' 'emotion' 'enjoys' 'every' 'expert'\n",
            " 'exploration' 'fantastic' 'field' 'flower' 'football' 'foster' 'fox'\n",
            " 'full' 'garden' 'great' 'healthy' 'help' 'history' 'horizon' 'human'\n",
            " 'improves' 'influence' 'innovation' 'inspired' 'inspires' 'intelligence'\n",
            " 'interdisciplinary' 'italian' 'jump' 'key' 'language' 'launched' 'lazy'\n",
            " 'lead' 'learning' 'life' 'love' 'machine' 'making' 'meditation' 'movie'\n",
            " 'music' 'natural' 'network' 'neural' 'new' 'open' 'play' 'processing'\n",
            " 'product' 'programming' 'provides' 'python' 'quick' 'rapidly' 'reading'\n",
            " 'reduce' 'resource' 'scalable' 'science' 'software' 'source' 'space'\n",
            " 'stress' 'success' 'sunny' 'technology' 'thrilling' 'today'\n",
            " 'transforming' 'traveling' 'visualization' 'vocabulary' 'warm' 'weather'\n",
            " 'weekend' 'world']\n",
            "Matriz BoW (dimensão): (25, 99)\n",
            "    advance  analytics  artificial  better  big  blooming  book  brain  \\\n",
            "0         0          0           0       0    0         0     0      0   \n",
            "1         0          0           0       0    0         0     0      0   \n",
            "2         0          0           0       0    0         0     0      0   \n",
            "3         0          0           0       0    0         0     0      0   \n",
            "4         0          0           1       0    0         0     0      0   \n",
            "5         0          0           0       0    0         0     0      0   \n",
            "6         0          0           0       0    0         0     1      0   \n",
            "7         0          0           0       0    0         0     0      0   \n",
            "8         0          0           0       0    0         0     0      0   \n",
            "9         0          1           0       0    1         0     0      0   \n",
            "10        0          0           0       0    0         0     0      1   \n",
            "11        0          0           0       0    0         0     0      0   \n",
            "12        0          0           0       0    0         0     0      0   \n",
            "13        0          0           0       0    0         0     0      0   \n",
            "14        0          0           0       1    0         0     0      0   \n",
            "15        0          0           0       0    0         0     0      0   \n",
            "16        0          0           0       0    0         0     0      0   \n",
            "17        0          0           0       0    0         0     0      0   \n",
            "18        0          0           0       0    0         0     0      0   \n",
            "19        1          0           0       0    0         0     0      0   \n",
            "20        0          0           0       0    0         0     0      0   \n",
            "21        0          0           0       0    0         0     0      0   \n",
            "22        0          0           0       0    0         0     0      0   \n",
            "23        0          0           0       0    0         1     0      0   \n",
            "24        0          0           0       0    0         0     0      0   \n",
            "\n",
            "    broadens  brown  ...  thrilling  today  transforming  traveling  \\\n",
            "0          0      1  ...          0      0             0          0   \n",
            "1          0      0  ...          0      0             0          0   \n",
            "2          0      0  ...          0      0             0          0   \n",
            "3          0      0  ...          0      0             0          0   \n",
            "4          0      0  ...          0      0             1          0   \n",
            "5          0      0  ...          0      1             0          0   \n",
            "6          0      0  ...          0      0             0          0   \n",
            "7          0      0  ...          0      0             0          0   \n",
            "8          0      0  ...          0      0             0          0   \n",
            "9          0      0  ...          0      0             0          0   \n",
            "10         0      0  ...          0      0             0          0   \n",
            "11         0      0  ...          1      0             0          0   \n",
            "12         0      0  ...          0      0             0          0   \n",
            "13         1      0  ...          0      0             0          1   \n",
            "14         0      0  ...          0      0             0          0   \n",
            "15         0      0  ...          0      0             0          0   \n",
            "16         0      0  ...          0      0             0          0   \n",
            "17         0      0  ...          0      0             0          0   \n",
            "18         0      0  ...          0      0             0          0   \n",
            "19         0      0  ...          0      0             0          0   \n",
            "20         0      0  ...          0      0             0          0   \n",
            "21         0      0  ...          0      0             0          0   \n",
            "22         0      0  ...          0      0             0          0   \n",
            "23         0      0  ...          0      0             0          0   \n",
            "24         0      0  ...          0      0             0          0   \n",
            "\n",
            "    visualization  vocabulary  warm  weather  weekend  world  \n",
            "0               0           0     0        0        0      0  \n",
            "1               0           0     0        0        0      0  \n",
            "2               0           0     0        0        0      0  \n",
            "3               0           0     0        0        0      0  \n",
            "4               0           0     0        0        0      1  \n",
            "5               0           0     1        1        0      0  \n",
            "6               0           0     0        0        0      0  \n",
            "7               0           0     0        0        0      0  \n",
            "8               0           0     0        0        0      0  \n",
            "9               0           0     0        0        0      0  \n",
            "10              0           0     0        0        0      0  \n",
            "11              0           0     0        0        0      0  \n",
            "12              0           0     0        0        1      0  \n",
            "13              0           0     0        0        0      0  \n",
            "14              0           0     0        0        0      0  \n",
            "15              0           0     0        0        0      0  \n",
            "16              0           0     0        0        0      0  \n",
            "17              0           0     0        0        0      0  \n",
            "18              0           0     0        0        0      0  \n",
            "19              0           0     0        0        0      0  \n",
            "20              0           0     0        0        0      0  \n",
            "21              0           1     0        0        0      0  \n",
            "22              1           0     0        0        0      0  \n",
            "23              0           0     0        0        0      0  \n",
            "24              0           0     0        0        0      0  \n",
            "\n",
            "[25 rows x 99 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Bag-of-Words para português\n",
        "print(\"\\n--- Bag of Words - Português ---\")\n",
        "vetorizador_pt = CountVectorizer()\n",
        "X_pt = vetorizador_pt.fit_transform(portugues_processado)\n",
        "print(\"Vocabulário português:\", vetorizador_pt.get_feature_names_out())\n",
        "print(\"Matriz BoW (dimensão):\", X_pt.shape)\n",
        "print(pd.DataFrame(X_pt.toarray(), columns=vetorizador_pt.get_feature_names_out()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHKKThyCOgtu",
        "outputId": "eae170cc-e170-4405-8972-30f2ab01ec46"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Bag of Words - Português ---\n",
            "Vocabulário português: ['abert' 'ajud' 'aliment' 'amo' 'ampl' 'anális' 'aprend' 'artific' 'avanç'\n",
            " 'big' 'camp' 'chav' 'chei' 'ciênc' 'colabor' 'compreens' 'comput'\n",
            " 'cozinh' 'culinár' 'cão' 'cérebr' 'códig' 'dad' 'dat' 'decis' 'desabroch'\n",
            " 'dia' 'educ' 'emocion' 'emoç' 'empr' 'ensolar' 'escal' 'espac' 'especi'\n",
            " 'estr' 'explor' 'fant' 'film' 'fim' 'fl' 'futebol' 'gost' 'histór' 'hoj'\n",
            " 'horizont' 'human' 'influenci' 'inov' 'insp' 'inspir' 'intelig'\n",
            " 'interdisciplin' 'itali' 'jardim' 'jog' 'lanç' 'ler' 'lev' 'lingu' 'livr'\n",
            " 'marrom' 'medit' 'melhor' 'mund' 'máquin' 'músic' 'natur' 'neur' 'nov'\n",
            " 'nuv' 'oferec' 'pod' 'preguiç' 'process' 'produt' 'program' 'promov'\n",
            " 'pul' 'python' 'quent' 'rap' 'rapid' 'recurs' 'red' 'reduz' 'rápid'\n",
            " 'saud' 'seman' 'sobr' 'softw' 'sucess' 'tecnolog' 'temp' 'tod' 'tom'\n",
            " 'transform' 'viaj' 'vid' 'visual' 'vocabul' 'ótim']\n",
            "Matriz BoW (dimensão): (25, 102)\n",
            "    abert  ajud  aliment  amo  ampl  anális  aprend  artific  avanç  big  ...  \\\n",
            "0       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "1       0     0        0    1     0       0       1        0      0    0  ...   \n",
            "2       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "3       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "4       0     0        0    0     0       0       0        1      0    0  ...   \n",
            "5       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "6       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "7       1     0        0    0     0       0       0        0      0    0  ...   \n",
            "8       0     0        0    0     0       0       0        0      0    0  ...   \n",
            "9       0     1        0    0     0       1       0        0      0    1  ...   \n",
            "10      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "11      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "12      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "13      0     0        0    0     1       0       0        0      0    0  ...   \n",
            "14      0     0        1    0     0       0       0        0      0    0  ...   \n",
            "15      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "16      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "17      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "18      0     0        0    0     0       0       1        0      0    0  ...   \n",
            "19      0     0        0    0     0       0       0        0      1    0  ...   \n",
            "20      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "21      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "22      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "23      0     0        0    0     0       0       0        0      0    0  ...   \n",
            "24      0     1        0    0     0       0       0        0      0    0  ...   \n",
            "\n",
            "    tecnolog  temp  tod  tom  transform  viaj  vid  visual  vocabul  ótim  \n",
            "0          0     0    0    0          0     0    0       0        0     0  \n",
            "1          0     0    0    0          0     0    0       0        0     0  \n",
            "2          0     0    0    0          0     0    0       0        0     1  \n",
            "3          0     0    0    0          0     0    0       0        0     0  \n",
            "4          0     0    0    0          1     0    0       0        0     0  \n",
            "5          0     1    0    0          0     0    0       0        0     0  \n",
            "6          0     0    0    0          0     0    0       0        0     0  \n",
            "7          0     0    0    0          0     0    0       0        0     0  \n",
            "8          0     0    0    0          0     0    0       0        0     0  \n",
            "9          0     0    0    1          0     0    0       0        0     0  \n",
            "10         0     0    0    0          0     0    0       0        0     0  \n",
            "11         0     0    0    0          0     0    0       0        0     0  \n",
            "12         0     0    1    0          0     0    0       0        0     0  \n",
            "13         0     0    0    0          0     1    0       0        0     0  \n",
            "14         0     0    0    0          0     0    1       0        0     0  \n",
            "15         0     0    0    0          0     0    0       0        0     0  \n",
            "16         0     0    0    0          0     0    0       0        0     0  \n",
            "17         0     0    0    0          0     0    0       0        0     0  \n",
            "18         0     0    0    0          0     0    0       0        0     0  \n",
            "19         1     0    0    0          0     0    0       0        0     0  \n",
            "20         0     0    0    0          0     0    0       0        0     0  \n",
            "21         0     0    0    0          0     0    0       0        1     0  \n",
            "22         0     0    0    0          0     0    0       1        0     0  \n",
            "23         0     0    0    0          0     0    0       0        0     0  \n",
            "24         0     0    0    0          0     0    0       0        0     0  \n",
            "\n",
            "[25 rows x 102 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No scikit-learn, por exemplo, ao usar CountVectorizer, o vocabulário é criado automaticamente ao chamar o método .fit() ou .fit_transform() nos textos. Você pode acessar esse vocabulário com:"
      ],
      "metadata": {
        "id": "5jiMEXWZOhbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste de Como vetorizar um conjunto de Palavras:"
      ],
      "metadata": {
        "id": "4zkMAeSre3zd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora usaremos  SkLearn apenas para vetorizar, mesmo sem pré processamento como no  Kaggle só para ver como ficam frases em ingles:"
      ],
      "metadata": {
        "id": "5yb9gIM1ZCVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Documentos\n",
        "text = [ \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I love machine learning and natural language processing.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"Data science is an interdisciplinary field.\",\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"She enjoys reading books on history.\",\n",
        "    \"Open source software fosters collaboration.\",\n",
        "    \"Cloud computing provides scalable resources.\",\n",
        "    \"Big data analytics helps in decision making.\",\n",
        "    \"Neural networks are inspired by the human brain.\",\n",
        "    \"The movie was fantastic and thrilling.\",\n",
        "    \"He plays football every weekend.\",\n",
        "    \"Traveling broadens your horizons.\",\n",
        "    \"Healthy eating leads to a better life.\",\n",
        "    \"Education is the key to success.\",\n",
        "    \"Music can influence our emotions.\",\n",
        "    \"The company launched a new product.\",\n",
        "    \"I am learning to cook Italian cuisine.\",\n",
        "    \"Technology advances rapidly these days.\",\n",
        "    \"Space exploration inspires innovation.\",\n",
        "    \"Reading improves vocabulary and comprehension.\",\n",
        "    \"She is an expert in data visualization.\",\n",
        "    \"The garden is full of blooming flowers.\",\n",
        "    \"Meditation helps reduce stress.\"\n",
        "]\n",
        "# Vetorização\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(text)\n",
        "\n",
        "# Exibição como DataFrame\n",
        "df_bow = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out(),\n",
        "    index=[f\"Doc {i+1}\" for i in range(len(text))]\n",
        ")\n",
        "print(df_bow)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sbb-9xpaSN0",
        "outputId": "87b26d84-469b-41db-b2e7-8af25f72967e"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        advances  am  an  analytics  and  are  artificial  better  big  \\\n",
            "Doc 1          0   0   0          0    0    0           0       0    0   \n",
            "Doc 2          0   0   0          0    1    0           0       0    0   \n",
            "Doc 3          0   0   0          0    0    0           0       0    0   \n",
            "Doc 4          0   0   1          0    0    0           0       0    0   \n",
            "Doc 5          0   0   0          0    0    0           1       0    0   \n",
            "Doc 6          0   0   0          0    1    0           0       0    0   \n",
            "Doc 7          0   0   0          0    0    0           0       0    0   \n",
            "Doc 8          0   0   0          0    0    0           0       0    0   \n",
            "Doc 9          0   0   0          0    0    0           0       0    0   \n",
            "Doc 10         0   0   0          1    0    0           0       0    1   \n",
            "Doc 11         0   0   0          0    0    1           0       0    0   \n",
            "Doc 12         0   0   0          0    1    0           0       0    0   \n",
            "Doc 13         0   0   0          0    0    0           0       0    0   \n",
            "Doc 14         0   0   0          0    0    0           0       0    0   \n",
            "Doc 15         0   0   0          0    0    0           0       1    0   \n",
            "Doc 16         0   0   0          0    0    0           0       0    0   \n",
            "Doc 17         0   0   0          0    0    0           0       0    0   \n",
            "Doc 18         0   0   0          0    0    0           0       0    0   \n",
            "Doc 19         0   1   0          0    0    0           0       0    0   \n",
            "Doc 20         1   0   0          0    0    0           0       0    0   \n",
            "Doc 21         0   0   0          0    0    0           0       0    0   \n",
            "Doc 22         0   0   0          0    1    0           0       0    0   \n",
            "Doc 23         0   0   1          0    0    0           0       0    0   \n",
            "Doc 24         0   0   0          0    0    0           0       0    0   \n",
            "Doc 25         0   0   0          0    0    0           0       0    0   \n",
            "\n",
            "        blooming  ...  transforming  traveling  visualization  vocabulary  \\\n",
            "Doc 1          0  ...             0          0              0           0   \n",
            "Doc 2          0  ...             0          0              0           0   \n",
            "Doc 3          0  ...             0          0              0           0   \n",
            "Doc 4          0  ...             0          0              0           0   \n",
            "Doc 5          0  ...             1          0              0           0   \n",
            "Doc 6          0  ...             0          0              0           0   \n",
            "Doc 7          0  ...             0          0              0           0   \n",
            "Doc 8          0  ...             0          0              0           0   \n",
            "Doc 9          0  ...             0          0              0           0   \n",
            "Doc 10         0  ...             0          0              0           0   \n",
            "Doc 11         0  ...             0          0              0           0   \n",
            "Doc 12         0  ...             0          0              0           0   \n",
            "Doc 13         0  ...             0          0              0           0   \n",
            "Doc 14         0  ...             0          1              0           0   \n",
            "Doc 15         0  ...             0          0              0           0   \n",
            "Doc 16         0  ...             0          0              0           0   \n",
            "Doc 17         0  ...             0          0              0           0   \n",
            "Doc 18         0  ...             0          0              0           0   \n",
            "Doc 19         0  ...             0          0              0           0   \n",
            "Doc 20         0  ...             0          0              0           0   \n",
            "Doc 21         0  ...             0          0              0           0   \n",
            "Doc 22         0  ...             0          0              0           1   \n",
            "Doc 23         0  ...             0          0              1           0   \n",
            "Doc 24         1  ...             0          0              0           0   \n",
            "Doc 25         0  ...             0          0              0           0   \n",
            "\n",
            "        warm  was  weather  weekend  world  your  \n",
            "Doc 1      0    0        0        0      0     0  \n",
            "Doc 2      0    0        0        0      0     0  \n",
            "Doc 3      0    0        0        0      0     0  \n",
            "Doc 4      0    0        0        0      0     0  \n",
            "Doc 5      0    0        0        0      1     0  \n",
            "Doc 6      1    0        1        0      0     0  \n",
            "Doc 7      0    0        0        0      0     0  \n",
            "Doc 8      0    0        0        0      0     0  \n",
            "Doc 9      0    0        0        0      0     0  \n",
            "Doc 10     0    0        0        0      0     0  \n",
            "Doc 11     0    0        0        0      0     0  \n",
            "Doc 12     0    1        0        0      0     0  \n",
            "Doc 13     0    0        0        1      0     0  \n",
            "Doc 14     0    0        0        0      0     1  \n",
            "Doc 15     0    0        0        0      0     0  \n",
            "Doc 16     0    0        0        0      0     0  \n",
            "Doc 17     0    0        0        0      0     0  \n",
            "Doc 18     0    0        0        0      0     0  \n",
            "Doc 19     0    0        0        0      0     0  \n",
            "Doc 20     0    0        0        0      0     0  \n",
            "Doc 21     0    0        0        0      0     0  \n",
            "Doc 22     0    0        0        0      0     0  \n",
            "Doc 23     0    0        0        0      0     0  \n",
            "Doc 24     0    0        0        0      0     0  \n",
            "Doc 25     0    0        0        0      0     0  \n",
            "\n",
            "[25 rows x 118 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nota-se que não adianta fazer um BOW sem pré processamento."
      ],
      "metadata": {
        "id": "kkVIXBu7jsL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agora vamos ao BOW feito de forma manual com pré processamento :"
      ],
      "metadata": {
        "id": "oiFE1MEPexPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Documentação do Script Bag‑of‑Words (BoW)\n",
        "\n",
        "Este documento descreve as principais **funções** e **passos** do script que implementa um Bag‑of‑Words tanto de forma manual quanto usando o `CountVectorizer` do scikit‑learn.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Definição do Corpus (`frases_ingles`)\n",
        "- **Objetivo**: Armazenar as 25 frases em inglês que serão processadas.\n",
        "- **Uso**: Substitua ou ajuste esta lista para alterar o conjunto de documentos.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Função `preprocessar_texto(texto: str) -> list[str]`\n",
        "Realiza o **pré‑processamento** de uma frase, retornando uma lista de tokens prontos para contagem.  \n",
        "1. **Lowercase**  \n",
        "   - Converte toda string para letras minúsculas.  \n",
        "2. **Remoção de pontuação**  \n",
        "   - Elimina caracteres que não sejam letras ou números, usando expressão regular.  \n",
        "3. **Extração de tokens**  \n",
        "   - Utiliza `re.findall(r'\\b\\w+\\b', texto)` para obter apenas palavras alfanuméricas.  \n",
        "4. **Filtragem de stopwords**  \n",
        "   - Descarta termos muito frequentes que pouco agregam (lista interna simplificada).  \n",
        "5. **Lematização básica**  \n",
        "   - Remove “s” final de plurais (método simples, sem dependências externas).\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Aplicação do Pré‑processamento (`textos_processados`)\n",
        "- **Processo**: Para cada frase em `frases_ingles`, chama `preprocessar_texto` e junta os tokens com espaços.\n",
        "- **Resultado**: Lista de strings “limpas” e tokenizadas, prontas para vetorização.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Construção Manual do Vocabulário (`vocabulario`)\n",
        "- **Método**:  \n",
        "  1. Une todos os tokens de todas as frases em um **conjunto** (remove duplicatas).  \n",
        "  2. Converte em lista **ordenada** alfabeticamente.  \n",
        "- **Produto**: Vetor de palavras únicas que define as **colunas** da matriz BoW.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Montagem da Matriz BoW Manual (`df_manual`)\n",
        "- **Como funciona**:  \n",
        "  - Cada **linha** corresponde a uma frase.  \n",
        "  - Cada **coluna** corresponde a uma palavra do `vocabulario`.  \n",
        "  - A **célula (i, j)** é a contagem de quantas vezes a palavra `j` aparece na frase `i`.  \n",
        "- **Ferramenta**: `pandas.DataFrame` para facilitar visualização e conferência.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Vetorização Automática com `CountVectorizer` (`df_sklearn`)\n",
        "- **Configuração**:  \n",
        "  - `vocabulary=vocabulario`: garante que o sklearn use o mesmo conjunto e ordem de palavras.  \n",
        "  - `token_pattern` e `lowercase`: replicam a lógica manual de extração de tokens.  \n",
        "- **Execução**: `fit_transform(textos_processados)` produz a matriz BoW.\n",
        "- **Comparação**: `X.toarray()` deve coincidir exatamente com a `df_manual`.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Verificação Final\n",
        "- **Objetivo**: Confirmar que os **vocabulários** construídos manualmente e pelo sklearn são idênticos.\n",
        "- **Como**: Imprime tanto `vocabulario` quanto `vetorizador.get_feature_names_out()`.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusão\n",
        "Este script demonstra passo a passo como:\n",
        "1. **Pré‑processar** texto sem depender de bibliotecas.  \n",
        "2. **Construir** manualmente um Bag‑of‑Words.  \n",
        "3. **Automatizar** o mesmo processo com `CountVectorizer`.  \n",
        "\n",
        "Dessa forma, fica clara a equivalência entre o método “na mão” e a implementação de biblioteca.\n"
      ],
      "metadata": {
        "id": "pznf2huceR_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Lista das suas 25 frases em inglês\n",
        "frases_ingles = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I love machine learning and natural language processing.\",\n",
        "    \"Python is a great programming language.\",\n",
        "    \"Data science is an interdisciplinary field.\",\n",
        "    \"Artificial intelligence is transforming the world.\",\n",
        "    \"The weather today is sunny and warm.\",\n",
        "    \"She enjoys reading books on history.\",\n",
        "    \"Open source software fosters collaboration.\",\n",
        "    \"Cloud computing provides scalable resources.\",\n",
        "    \"Big data analytics helps in decision making.\",\n",
        "    \"Neural networks are inspired by the human brain.\",\n",
        "    \"The movie was fantastic and thrilling.\",\n",
        "    \"He plays football every weekend.\",\n",
        "    \"Traveling broadens your horizons.\",\n",
        "    \"Healthy eating leads to a better life.\",\n",
        "    \"Education is the key to success.\",\n",
        "    \"Music can influence our emotions.\",\n",
        "    \"The company launched a new product.\",\n",
        "    \"I am learning to cook Italian cuisine.\",\n",
        "    \"Technology advances rapidly these days.\",\n",
        "    \"Space exploration inspires innovation.\",\n",
        "    \"Reading improves vocabulary and comprehension.\",\n",
        "    \"She is an expert in data visualization.\",\n",
        "    \"The garden is full of blooming flowers.\",\n",
        "    \"Meditation helps reduce stress.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "_JY72RFthe6g"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Função de pré‑processamento completa\n",
        "def preprocessar_texto(texto: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Recebe uma string e retorna lista de tokens processados:\n",
        "      1. Converte para minúsculas\n",
        "      2. Remove pontuação\n",
        "      3. Extrai tokens via regex (palavras alfanuméricas)\n",
        "      4. Remove stopwords (lista interna)\n",
        "      5. Lematiza cada token (usando forma simples, sem NLTK)\n",
        "    \"\"\"\n",
        "    # lista simples de stopwords\n",
        "    stopwords_ingles = {\n",
        "        \"the\",\"and\",\"is\",\"in\",\"to\",\"a\",\"an\",\"of\",\"for\",\"on\",\"by\",\"that\",\n",
        "        \"this\",\"with\",\"as\",\"at\",\"it\",\"from\",\"or\",\"be\",\"are\",\"was\",\"were\"\n",
        "    }\n",
        "    # lowercase\n",
        "    texto = texto.lower()\n",
        "    # remove pontuação\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "    # extrai tokens alfanuméricos\n",
        "    tokens = re.findall(r'\\b\\w+\\b', texto)\n",
        "    # remove stopwords e aplica lematização simples (removendo 's' de plural)\n",
        "    tokens_filtrados = []\n",
        "    for tok in tokens:\n",
        "        if tok not in stopwords_ingles:\n",
        "            # lematização básica: se palavra termina em 's' e não é só 's', remove o 's'\n",
        "            if tok.endswith('s') and len(tok) > 2:\n",
        "                tok = tok[:-1]\n",
        "            tokens_filtrados.append(tok)\n",
        "    return tokens_filtrados\n",
        "    print(tokens_filtrados)"
      ],
      "metadata": {
        "id": "8yWXweoRhibU"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo faremos o vocabulario que é simplesmente a lista de todas as palavras únicas (tokens) que aparecem em todo o seu conjunto de textos processados. Em outras palavras:\n",
        "\n",
        "Coleta: para cada frase você já pré‑processada, você quebrou em tokens (“palavras”) e removeu repetições.\n",
        "\n",
        "Unicidade: ao usar um set ({…}), garante que cada token aparece apenas uma vez, mesmo que ocorra múltiplas vezes nas frases.\n",
        "\n",
        "Ordenação: ao aplicar sorted(), você transforma esse conjunto num vetor ordenado (alfabeticamente)."
      ],
      "metadata": {
        "id": "jq1RYRs1iMsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Aplicar pré‑processamento e recompor strings limpas\n",
        "textos_processados = [\n",
        "    \" \".join(preprocessar_texto(frase))\n",
        "    for frase in frases_ingles\n",
        "]\n",
        "\n",
        "# 4. Construir vocabulário manual (tokens únicos, ordenados)\n",
        "vocabulario = sorted({\n",
        "    token for texto in textos_processados for token in texto.split()\n",
        "})\n",
        "\n",
        "# 5. Montar matriz BoW manual\n",
        "matriz_manual = [\n",
        "    [texto.split().count(palavra) for palavra in vocabulario]\n",
        "    for texto in textos_processados\n",
        "]\n",
        "\n",
        "df_manual = pd.DataFrame(\n",
        "    matriz_manual,\n",
        "    columns=vocabulario,\n",
        "    index=[f\"Frase {i+1}\" for i in range(len(frases_ingles))]\n",
        ")\n",
        "\n",
        "print(\"=== Matriz BoW manual (após pré‑processamento) ===\")\n",
        "print(df_manual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_S2myL-hy67",
        "outputId": "dd542fe1-3b18-4f67-cd3c-c87091a6fa03"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Matriz BoW manual (após pré‑processamento) ===\n",
            "          advance  am  analytic  artificial  better  big  blooming  book  \\\n",
            "Frase 1         0   0         0           0       0    0         0     0   \n",
            "Frase 2         0   0         0           0       0    0         0     0   \n",
            "Frase 3         0   0         0           0       0    0         0     0   \n",
            "Frase 4         0   0         0           0       0    0         0     0   \n",
            "Frase 5         0   0         0           1       0    0         0     0   \n",
            "Frase 6         0   0         0           0       0    0         0     0   \n",
            "Frase 7         0   0         0           0       0    0         0     1   \n",
            "Frase 8         0   0         0           0       0    0         0     0   \n",
            "Frase 9         0   0         0           0       0    0         0     0   \n",
            "Frase 10        0   0         1           0       0    1         0     0   \n",
            "Frase 11        0   0         0           0       0    0         0     0   \n",
            "Frase 12        0   0         0           0       0    0         0     0   \n",
            "Frase 13        0   0         0           0       0    0         0     0   \n",
            "Frase 14        0   0         0           0       0    0         0     0   \n",
            "Frase 15        0   0         0           0       1    0         0     0   \n",
            "Frase 16        0   0         0           0       0    0         0     0   \n",
            "Frase 17        0   0         0           0       0    0         0     0   \n",
            "Frase 18        0   0         0           0       0    0         0     0   \n",
            "Frase 19        0   1         0           0       0    0         0     0   \n",
            "Frase 20        1   0         0           0       0    0         0     0   \n",
            "Frase 21        0   0         0           0       0    0         0     0   \n",
            "Frase 22        0   0         0           0       0    0         0     0   \n",
            "Frase 23        0   0         0           0       0    0         0     0   \n",
            "Frase 24        0   0         0           0       0    0         1     0   \n",
            "Frase 25        0   0         0           0       0    0         0     0   \n",
            "\n",
            "          brain  broaden  ...  today  transforming  traveling  visualization  \\\n",
            "Frase 1       0        0  ...      0             0          0              0   \n",
            "Frase 2       0        0  ...      0             0          0              0   \n",
            "Frase 3       0        0  ...      0             0          0              0   \n",
            "Frase 4       0        0  ...      0             0          0              0   \n",
            "Frase 5       0        0  ...      0             1          0              0   \n",
            "Frase 6       0        0  ...      1             0          0              0   \n",
            "Frase 7       0        0  ...      0             0          0              0   \n",
            "Frase 8       0        0  ...      0             0          0              0   \n",
            "Frase 9       0        0  ...      0             0          0              0   \n",
            "Frase 10      0        0  ...      0             0          0              0   \n",
            "Frase 11      1        0  ...      0             0          0              0   \n",
            "Frase 12      0        0  ...      0             0          0              0   \n",
            "Frase 13      0        0  ...      0             0          0              0   \n",
            "Frase 14      0        1  ...      0             0          1              0   \n",
            "Frase 15      0        0  ...      0             0          0              0   \n",
            "Frase 16      0        0  ...      0             0          0              0   \n",
            "Frase 17      0        0  ...      0             0          0              0   \n",
            "Frase 18      0        0  ...      0             0          0              0   \n",
            "Frase 19      0        0  ...      0             0          0              0   \n",
            "Frase 20      0        0  ...      0             0          0              0   \n",
            "Frase 21      0        0  ...      0             0          0              0   \n",
            "Frase 22      0        0  ...      0             0          0              0   \n",
            "Frase 23      0        0  ...      0             0          0              1   \n",
            "Frase 24      0        0  ...      0             0          0              0   \n",
            "Frase 25      0        0  ...      0             0          0              0   \n",
            "\n",
            "          vocabulary  warm  weather  weekend  world  your  \n",
            "Frase 1            0     0        0        0      0     0  \n",
            "Frase 2            0     0        0        0      0     0  \n",
            "Frase 3            0     0        0        0      0     0  \n",
            "Frase 4            0     0        0        0      0     0  \n",
            "Frase 5            0     0        0        0      1     0  \n",
            "Frase 6            0     1        1        0      0     0  \n",
            "Frase 7            0     0        0        0      0     0  \n",
            "Frase 8            0     0        0        0      0     0  \n",
            "Frase 9            0     0        0        0      0     0  \n",
            "Frase 10           0     0        0        0      0     0  \n",
            "Frase 11           0     0        0        0      0     0  \n",
            "Frase 12           0     0        0        0      0     0  \n",
            "Frase 13           0     0        0        1      0     0  \n",
            "Frase 14           0     0        0        0      0     1  \n",
            "Frase 15           0     0        0        0      0     0  \n",
            "Frase 16           0     0        0        0      0     0  \n",
            "Frase 17           0     0        0        0      0     0  \n",
            "Frase 18           0     0        0        0      0     0  \n",
            "Frase 19           0     0        0        0      0     0  \n",
            "Frase 20           0     0        0        0      0     0  \n",
            "Frase 21           0     0        0        0      0     0  \n",
            "Frase 22           1     0        0        0      0     0  \n",
            "Frase 23           0     0        0        0      0     0  \n",
            "Frase 24           0     0        0        0      0     0  \n",
            "Frase 25           0     0        0        0      0     0  \n",
            "\n",
            "[25 rows x 108 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# 6. Gerar Bag‑of‑Words com sklearn usando o mesmo vocabulário\n",
        "vetorizador = CountVectorizer(\n",
        "    vocabulary=vocabulario,\n",
        "    lowercase=True,\n",
        "    token_pattern=r\"(?u)\\b\\w+\\b\"  # usa mesma lógica de extração\n",
        ")\n",
        "X = vetorizador.fit_transform(textos_processados)\n",
        "\n",
        "df_sklearn = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "    columns=vetorizador.get_feature_names_out(),\n",
        "    index=[f\"Frase {i+1}\" for i in range(len(frases_ingles))]\n",
        ")\n",
        "\n",
        "print(\"\\n=== Matriz BoW com sklearn (após pré‑processamento) ===\")\n",
        "print(df_sklearn)\n",
        "\n",
        "# 7. Verificação de vocabulários\n",
        "print(\"\\nVocabulário manual   :\", vocabulario)\n",
        "print(\"Vocabulário sklearn  :\", list(vetorizador.get_feature_names_out()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVBvXR1NcWTS",
        "outputId": "c9997430-7863-4a6f-a84b-ca2b4d6f261d"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Matriz BoW com sklearn (após pré‑processamento) ===\n",
            "          advance  am  analytic  artificial  better  big  blooming  book  \\\n",
            "Frase 1         0   0         0           0       0    0         0     0   \n",
            "Frase 2         0   0         0           0       0    0         0     0   \n",
            "Frase 3         0   0         0           0       0    0         0     0   \n",
            "Frase 4         0   0         0           0       0    0         0     0   \n",
            "Frase 5         0   0         0           1       0    0         0     0   \n",
            "Frase 6         0   0         0           0       0    0         0     0   \n",
            "Frase 7         0   0         0           0       0    0         0     1   \n",
            "Frase 8         0   0         0           0       0    0         0     0   \n",
            "Frase 9         0   0         0           0       0    0         0     0   \n",
            "Frase 10        0   0         1           0       0    1         0     0   \n",
            "Frase 11        0   0         0           0       0    0         0     0   \n",
            "Frase 12        0   0         0           0       0    0         0     0   \n",
            "Frase 13        0   0         0           0       0    0         0     0   \n",
            "Frase 14        0   0         0           0       0    0         0     0   \n",
            "Frase 15        0   0         0           0       1    0         0     0   \n",
            "Frase 16        0   0         0           0       0    0         0     0   \n",
            "Frase 17        0   0         0           0       0    0         0     0   \n",
            "Frase 18        0   0         0           0       0    0         0     0   \n",
            "Frase 19        0   1         0           0       0    0         0     0   \n",
            "Frase 20        1   0         0           0       0    0         0     0   \n",
            "Frase 21        0   0         0           0       0    0         0     0   \n",
            "Frase 22        0   0         0           0       0    0         0     0   \n",
            "Frase 23        0   0         0           0       0    0         0     0   \n",
            "Frase 24        0   0         0           0       0    0         1     0   \n",
            "Frase 25        0   0         0           0       0    0         0     0   \n",
            "\n",
            "          brain  broaden  ...  today  transforming  traveling  visualization  \\\n",
            "Frase 1       0        0  ...      0             0          0              0   \n",
            "Frase 2       0        0  ...      0             0          0              0   \n",
            "Frase 3       0        0  ...      0             0          0              0   \n",
            "Frase 4       0        0  ...      0             0          0              0   \n",
            "Frase 5       0        0  ...      0             1          0              0   \n",
            "Frase 6       0        0  ...      1             0          0              0   \n",
            "Frase 7       0        0  ...      0             0          0              0   \n",
            "Frase 8       0        0  ...      0             0          0              0   \n",
            "Frase 9       0        0  ...      0             0          0              0   \n",
            "Frase 10      0        0  ...      0             0          0              0   \n",
            "Frase 11      1        0  ...      0             0          0              0   \n",
            "Frase 12      0        0  ...      0             0          0              0   \n",
            "Frase 13      0        0  ...      0             0          0              0   \n",
            "Frase 14      0        1  ...      0             0          1              0   \n",
            "Frase 15      0        0  ...      0             0          0              0   \n",
            "Frase 16      0        0  ...      0             0          0              0   \n",
            "Frase 17      0        0  ...      0             0          0              0   \n",
            "Frase 18      0        0  ...      0             0          0              0   \n",
            "Frase 19      0        0  ...      0             0          0              0   \n",
            "Frase 20      0        0  ...      0             0          0              0   \n",
            "Frase 21      0        0  ...      0             0          0              0   \n",
            "Frase 22      0        0  ...      0             0          0              0   \n",
            "Frase 23      0        0  ...      0             0          0              1   \n",
            "Frase 24      0        0  ...      0             0          0              0   \n",
            "Frase 25      0        0  ...      0             0          0              0   \n",
            "\n",
            "          vocabulary  warm  weather  weekend  world  your  \n",
            "Frase 1            0     0        0        0      0     0  \n",
            "Frase 2            0     0        0        0      0     0  \n",
            "Frase 3            0     0        0        0      0     0  \n",
            "Frase 4            0     0        0        0      0     0  \n",
            "Frase 5            0     0        0        0      1     0  \n",
            "Frase 6            0     1        1        0      0     0  \n",
            "Frase 7            0     0        0        0      0     0  \n",
            "Frase 8            0     0        0        0      0     0  \n",
            "Frase 9            0     0        0        0      0     0  \n",
            "Frase 10           0     0        0        0      0     0  \n",
            "Frase 11           0     0        0        0      0     0  \n",
            "Frase 12           0     0        0        0      0     0  \n",
            "Frase 13           0     0        0        1      0     0  \n",
            "Frase 14           0     0        0        0      0     1  \n",
            "Frase 15           0     0        0        0      0     0  \n",
            "Frase 16           0     0        0        0      0     0  \n",
            "Frase 17           0     0        0        0      0     0  \n",
            "Frase 18           0     0        0        0      0     0  \n",
            "Frase 19           0     0        0        0      0     0  \n",
            "Frase 20           0     0        0        0      0     0  \n",
            "Frase 21           0     0        0        0      0     0  \n",
            "Frase 22           1     0        0        0      0     0  \n",
            "Frase 23           0     0        0        0      0     0  \n",
            "Frase 24           0     0        0        0      0     0  \n",
            "Frase 25           0     0        0        0      0     0  \n",
            "\n",
            "[25 rows x 108 columns]\n",
            "\n",
            "Vocabulário manual   : ['advance', 'am', 'analytic', 'artificial', 'better', 'big', 'blooming', 'book', 'brain', 'broaden', 'brown', 'can', 'cloud', 'collaboration', 'company', 'comprehension', 'computing', 'cook', 'cuisine', 'data', 'day', 'decision', 'dog', 'eating', 'education', 'emotion', 'enjoy', 'every', 'expert', 'exploration', 'fantastic', 'field', 'flower', 'football', 'foster', 'fox', 'full', 'garden', 'great', 'he', 'healthy', 'help', 'history', 'horizon', 'human', 'i', 'improve', 'influence', 'innovation', 'inspire', 'inspired', 'intelligence', 'interdisciplinary', 'italian', 'jump', 'key', 'language', 'launched', 'lazy', 'lead', 'learning', 'life', 'love', 'machine', 'making', 'meditation', 'movie', 'music', 'natural', 'network', 'neural', 'new', 'open', 'our', 'over', 'play', 'processing', 'product', 'programming', 'provide', 'python', 'quick', 'rapidly', 'reading', 'reduce', 'resource', 'scalable', 'science', 'she', 'software', 'source', 'space', 'stres', 'succes', 'sunny', 'technology', 'these', 'thrilling', 'today', 'transforming', 'traveling', 'visualization', 'vocabulary', 'warm', 'weather', 'weekend', 'world', 'your']\n",
            "Vocabulário sklearn  : ['advance', 'am', 'analytic', 'artificial', 'better', 'big', 'blooming', 'book', 'brain', 'broaden', 'brown', 'can', 'cloud', 'collaboration', 'company', 'comprehension', 'computing', 'cook', 'cuisine', 'data', 'day', 'decision', 'dog', 'eating', 'education', 'emotion', 'enjoy', 'every', 'expert', 'exploration', 'fantastic', 'field', 'flower', 'football', 'foster', 'fox', 'full', 'garden', 'great', 'he', 'healthy', 'help', 'history', 'horizon', 'human', 'i', 'improve', 'influence', 'innovation', 'inspire', 'inspired', 'intelligence', 'interdisciplinary', 'italian', 'jump', 'key', 'language', 'launched', 'lazy', 'lead', 'learning', 'life', 'love', 'machine', 'making', 'meditation', 'movie', 'music', 'natural', 'network', 'neural', 'new', 'open', 'our', 'over', 'play', 'processing', 'product', 'programming', 'provide', 'python', 'quick', 'rapidly', 'reading', 'reduce', 'resource', 'scalable', 'science', 'she', 'software', 'source', 'space', 'stres', 'succes', 'sunny', 'technology', 'these', 'thrilling', 'today', 'transforming', 'traveling', 'visualization', 'vocabulary', 'warm', 'weather', 'weekend', 'world', 'your']\n"
          ]
        }
      ]
    }
  ]
}